{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_full = pd.read_csv('train.csv', index_col='id')\n",
    "X_test_full = pd.read_csv('test_tweets.csv', index_col='id')\n",
    "#y_test=X_test_full[[\"label\"]]\n",
    "#X_test_full=X_test_full.drop([\"label\"],axis=1)\n",
    "y=X_full[[\"label\"]]\n",
    "X_full=X_full.drop([\"label\"],axis=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=1)\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31967</th>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31968</th>\n",
       "      <td>choose to be   :) #momtips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31969</th>\n",
       "      <td>something inside me dies ð¦ð¿â¨  eyes nes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31970</th>\n",
       "      <td>#finished#tattoo#inked#ink#loveitâ¤ï¸ #â¤ï¸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31971</th>\n",
       "      <td>@user @user @user i will never understand why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31972</th>\n",
       "      <td>#delicious   #food #lovelife #capetown mannaep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31973</th>\n",
       "      <td>1000dayswasted - narcosis infinite ep.. make m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31974</th>\n",
       "      <td>one of the world's greatest spoing events   #l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31975</th>\n",
       "      <td>half way through the website now and #allgoing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31976</th>\n",
       "      <td>good food, good life , #enjoy and   ððð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31977</th>\n",
       "      <td>i'll stand behind this #guncontrolplease   #se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31978</th>\n",
       "      <td>i ate,i ate and i ate...ðð   #jamaisasth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31979</th>\n",
       "      <td>@user got my @user limited edition rain or sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31980</th>\n",
       "      <td>&amp;amp; #love &amp;amp; #hugs &amp;amp; #kisses too! how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31981</th>\n",
       "      <td>ð­ðð #girls   #sun #fave @ london, uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31982</th>\n",
       "      <td>thought factory: bbc neutrality on right wing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31983</th>\n",
       "      <td>hey guys tommorow is the last day of my exams ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31984</th>\n",
       "      <td>@user @user  @user  #levyrroni #recuerdos mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31985</th>\n",
       "      <td>my mind is like ððð½ð but my body l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31986</th>\n",
       "      <td>never been this down on myself in my entire li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31987</th>\n",
       "      <td>check  twitterww - trends: \"trending worldwide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31988</th>\n",
       "      <td>i thought i saw a mermaid!!! #ceegee  #smcr   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31989</th>\n",
       "      <td>chick gets fucked hottest naked lady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31990</th>\n",
       "      <td>happy bday lucyâ¨â¨ð xoxo #love #beautifu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31991</th>\n",
       "      <td>haroldfriday have a weekend filled with sunbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31992</th>\n",
       "      <td>@user @user tried that! but nothing - will try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49130</th>\n",
       "      <td>people do anything for fucking attention nowad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49131</th>\n",
       "      <td>creative bubble got burst ð¢ looking forward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49132</th>\n",
       "      <td>tomorrow is gonna be a big day! we are going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49133</th>\n",
       "      <td>i am thankful for baby giggles. #thankful #pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49134</th>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49135</th>\n",
       "      <td>in life u will grow to learn some pple will wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49136</th>\n",
       "      <td>ði was the storm,you were the rain. togethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49137</th>\n",
       "      <td>lovelgq -  broken ep via   #rnb #love #heabrok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49138</th>\n",
       "      <td>spread love not hateâ¤ï¸ðððð #pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49139</th>\n",
       "      <td>@user @user are the most racist pay ever!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49140</th>\n",
       "      <td>i am thankful for children. #thankful #positiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49141</th>\n",
       "      <td>liverpool â¤ï¸ð¬ð§ #walk #liverpool #sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49142</th>\n",
       "      <td>#bakersfield   rooster simulation: i want to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49143</th>\n",
       "      <td>por do sol ó¾â¤ï¸#instagood #beautiful   #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49144</th>\n",
       "      <td>@user hell yeah what a great surprise for your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49145</th>\n",
       "      <td>when ur the joke ur defensive towards everythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49146</th>\n",
       "      <td>#enjoying the #evening #sun in my #bedroom â¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49147</th>\n",
       "      <td>tonight on @user from 9pm gmt  you can here a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49148</th>\n",
       "      <td>today is a good day for excercise #imready #so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49149</th>\n",
       "      <td>good night with a tea and music âï¸ðð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49150</th>\n",
       "      <td>loving lifeðºð¸âï¸ð  #createyourfutu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49151</th>\n",
       "      <td>black professor demonizes, proposes nazi style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49152</th>\n",
       "      <td>learn how to think positive.  #positive   #ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49153</th>\n",
       "      <td>we love the pretty, happy and fresh you! #teen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>2_damn_tuff-ruff_muff__techno_city-(ng005)-web...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>thought factory: left-right polarisation! #tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>feeling like a mermaid ð #hairflip #neverre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49159</th>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17197 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "id                                                      \n",
       "31963  #studiolife #aislife #requires #passion #dedic...\n",
       "31964   @user #white #supremacists want everyone to s...\n",
       "31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "31966  is the hp and the cursed child book up for res...\n",
       "31967    3rd #bihday to my amazing, hilarious #nephew...\n",
       "31968                        choose to be   :) #momtips \n",
       "31969  something inside me dies ð¦ð¿â¨  eyes nes...\n",
       "31970  #finished#tattoo#inked#ink#loveitâ¤ï¸ #â¤ï¸...\n",
       "31971   @user @user @user i will never understand why...\n",
       "31972  #delicious   #food #lovelife #capetown mannaep...\n",
       "31973  1000dayswasted - narcosis infinite ep.. make m...\n",
       "31974  one of the world's greatest spoing events   #l...\n",
       "31975  half way through the website now and #allgoing...\n",
       "31976  good food, good life , #enjoy and   ððð...\n",
       "31977  i'll stand behind this #guncontrolplease   #se...\n",
       "31978  i ate,i ate and i ate...ðð   #jamaisasth...\n",
       "31979   @user got my @user limited edition rain or sh...\n",
       "31980  &amp; #love &amp; #hugs &amp; #kisses too! how...\n",
       "31981  ð­ðð #girls   #sun #fave @ london, uni...\n",
       "31982  thought factory: bbc neutrality on right wing ...\n",
       "31983  hey guys tommorow is the last day of my exams ...\n",
       "31984   @user @user  @user  #levyrroni #recuerdos mem...\n",
       "31985  my mind is like ððð½ð but my body l...\n",
       "31986  never been this down on myself in my entire li...\n",
       "31987  check  twitterww - trends: \"trending worldwide...\n",
       "31988  i thought i saw a mermaid!!! #ceegee  #smcr   ...\n",
       "31989              chick gets fucked hottest naked lady \n",
       "31990  happy bday lucyâ¨â¨ð xoxo #love #beautifu...\n",
       "31991   haroldfriday have a weekend filled with sunbe...\n",
       "31992  @user @user tried that! but nothing - will try...\n",
       "...                                                  ...\n",
       "49130  people do anything for fucking attention nowad...\n",
       "49131  creative bubble got burst ð¢ looking forward...\n",
       "49132  tomorrow is gonna be a big day! we are going t...\n",
       "49133  i am thankful for baby giggles. #thankful #pos...\n",
       "49134  #model   i love u take with u all the time in ...\n",
       "49135  in life u will grow to learn some pple will wo...\n",
       "49136  ði was the storm,you were the rain. togethe...\n",
       "49137  lovelgq -  broken ep via   #rnb #love #heabrok...\n",
       "49138  spread love not hateâ¤ï¸ðððð #pr...\n",
       "49139     @user @user are the most racist pay ever!!!!! \n",
       "49140  i am thankful for children. #thankful #positiv...\n",
       "49141  liverpool â¤ï¸ð¬ð§ #walk #liverpool #sta...\n",
       "49142  #bakersfield   rooster simulation: i want to c...\n",
       "49143  por do sol ó¾â¤ï¸#instagood #beautiful   #...\n",
       "49144  @user hell yeah what a great surprise for your...\n",
       "49145  when ur the joke ur defensive towards everythi...\n",
       "49146  #enjoying the #evening #sun in my #bedroom â¨...\n",
       "49147  tonight on @user from 9pm gmt  you can here a ...\n",
       "49148  today is a good day for excercise #imready #so...\n",
       "49149  good night with a tea and music âï¸ðð...\n",
       "49150  loving lifeðºð¸âï¸ð  #createyourfutu...\n",
       "49151  black professor demonizes, proposes nazi style...\n",
       "49152  learn how to think positive.  #positive   #ins...\n",
       "49153  we love the pretty, happy and fresh you! #teen...\n",
       "49154  2_damn_tuff-ruff_muff__techno_city-(ng005)-web...\n",
       "49155  thought factory: left-right polarisation! #tru...\n",
       "49156  feeling like a mermaid ð #hairflip #neverre...\n",
       "49157  #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49158  happy, at work conference: right mindset leads...\n",
       "49159  my   song \"so glad\" free download!  #shoegaze ...\n",
       "\n",
       "[17197 rows x 1 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in X_train[\"tweet\"]:\n",
    "    tweet=textacy.preprocess_text(tweet,lowercase=True,no_punct=True,no_urls=True\n",
    "                                  ,no_emails=True,no_currency_symbols=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stop=list(spacy_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for tweet in X_train[\"tweet\"]:\n",
    "    doc=nlp(tweet)\n",
    "    #nos=[token.lemma_ for token in doc]\n",
    "    nos=[token.lemma_ for token in doc if not token.lemma_ in stop]\n",
    "    tweet=nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word.strip().lower() for word in mytokens if word.strip().lower() not in stop ]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for tweet in X_train[\"tweet\"]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "classifier = SVC(C=150, gamma=2e-2, probability=True)\n",
    "pipe=Pipeline([(\"vectorizer\",tfvectorizer),(\"classifier\",classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'worry', 'good', 'aâ\\x9d\\x8câ\\x9d\\x8c', 'ð\\x9f\\x90¶', 'niggaz', 'good', 'care', 'den', 'kid', '', 'ð\\x9f¤\\x94ð\\x9f\\x98\\x92']\n"
     ]
    }
   ],
   "source": [
    "for tweet in X_train[\"tweet\"]:\n",
    "    print(spacy_tokenizer(tweet))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25569, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got DataFrame)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d029e04150f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#y_train.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d368e2155e21>\u001b[0m in \u001b[0;36mspacy_tokenizer\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmytokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmytokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"-PRON-\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmytokens\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmytokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmytokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/desktop/python/jupyter1.0/jupyter1.0_env/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             )\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/desktop/python/jupyter1.0/jupyter1.0_env/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got DataFrame)"
     ]
    }
   ],
   "source": [
    "spacy_tokenizer(X_train)\n",
    "#y_train.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe.fit(X_train.values.reshape(-1,),y_train.values.reshape(-1,))\n",
    "pred=pipe.predict(X_valid.values.reshape(-1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7321212121212122"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_valid,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = pipe.predict(X_test_full.values.reshape(-1,))\n",
    "output = pd.DataFrame({'id': X_test_full.index,\n",
    "                       'label': preds_test})\n",
    "output.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
